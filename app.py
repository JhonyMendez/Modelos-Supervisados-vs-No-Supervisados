import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, silhouette_score, davies_bouldin_score)
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import json
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de la p√°gina
st.set_page_config(page_title="ML App - COVID-19 Pakistan", layout="wide")

# T√≠tulo principal
st.title("ü¶† Aplicaci√≥n de Machine Learning - COVID-19 Pakistan")
st.markdown("**Modelos:** Gaussian Naive Bayes (Supervisado) + K-Means (No Supervisado)")

# Cargar datos
@st.cache_data
def load_data():
    try:
        df = pd.read_csv('Pakistan_COVID19.csv')
        # Limpiar datos: eliminar filas con valores nulos
        df = df.dropna()
        return df
    except FileNotFoundError:
        st.error("‚ùå No se encontr√≥ el archivo 'Pakistan_COVID19.csv'. Por favor, col√≥calo en la misma carpeta que app.py")
        return None

df = load_data()

if df is None:
    st.stop()

# Preparar datos
@st.cache_data
def prepare_data(df):
    # Codificar la columna Province (variable categ√≥rica)
    le = LabelEncoder()
    df_processed = df.copy()
    df_processed['Province_Encoded'] = le.fit_transform(df_processed['Province'])
    
    # Caracter√≠sticas num√©ricas
    numeric_features = ['New_Cases', 'Recoveries', 'Deaths', 'Vaccinations', 'Hospitalized', 'Tests_Conducted']
    
    # Variable objetivo (target) ser√° Province_Encoded
    X = df_processed[numeric_features].values
    y = df_processed['Province_Encoded'].values
    
    return X, y, le, numeric_features, df_processed

X, y, label_encoder, feature_names, df_processed = prepare_data(df)
province_names = label_encoder.classes_

# Sidebar para navegaci√≥n
st.sidebar.title("üéØ Navegaci√≥n")
mode = st.sidebar.radio("Selecciona el Modo:", 
                        ["üìä Exploraci√≥n de Datos", 
                         "üéì Modelo Supervisado (Gaussian NB)", 
                         "üîç Modelo No Supervisado (K-Means)",
                         "üíæ Zona de Exportaci√≥n"])

# =============================================
# MODO 1: EXPLORACI√ìN DE DATOS
# =============================================
if mode == "üìä Exploraci√≥n de Datos":
    st.header("üìä Exploraci√≥n del Dataset COVID-19 Pakistan")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total de Registros", len(df))
    with col2:
        st.metric("Provincias", len(province_names))
    with col3:
        st.metric("Variables Num√©ricas", len(feature_names))
    with col4:
        st.metric("Total Casos", f"{df['New_Cases'].sum():,.0f}")
    
    st.subheader("Vista previa de los datos")
    st.dataframe(df.head(15))
    
    st.subheader("Informaci√≥n de las Columnas")
    col_info = pd.DataFrame({
        'Columna': df.columns,
        'Tipo': df.dtypes.values,
        'Valores No Nulos': df.count().values,
        'Valores √önicos': [df[col].nunique() for col in df.columns]
    })
    st.dataframe(col_info)
    
    st.subheader("Estad√≠sticas Descriptivas")
    st.dataframe(df[feature_names].describe())
    
    # Visualizaciones
    st.subheader("üìà Visualizaciones")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Distribuci√≥n de casos por provincia
        fig, ax = plt.subplots(figsize=(10, 6))
        province_cases = df.groupby('Province')['New_Cases'].sum().sort_values(ascending=False)
        province_cases.plot(kind='bar', ax=ax, color='#FF6B6B')
        ax.set_ylabel('Total de Casos Nuevos')
        ax.set_xlabel('Provincia')
        ax.set_title('Casos de COVID-19 por Provincia')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)
    
    with col2:
        # Distribuci√≥n de registros por provincia
        fig, ax = plt.subplots(figsize=(10, 6))
        df['Province'].value_counts().plot(kind='bar', ax=ax, color='#4ECDC4')
        ax.set_ylabel('N√∫mero de Registros')
        ax.set_xlabel('Provincia')
        ax.set_title('Distribuci√≥n de Registros por Provincia')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)
    
    # Matriz de correlaci√≥n
    st.subheader("üîó Matriz de Correlaci√≥n")
    fig, ax = plt.subplots(figsize=(10, 8))
    correlation_matrix = df[feature_names].corr()
    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, ax=ax, square=True)
    ax.set_title('Correlaci√≥n entre Variables')
    st.pyplot(fig)

# =============================================
# MODO 2: MODELO SUPERVISADO (GAUSSIAN NAIVE BAYES)
# =============================================
elif mode == "üéì Modelo Supervisado (Gaussian NB)":
    st.header("üéì Modelo Supervisado: Gaussian Naive Bayes")
    
    st.markdown("""
    **Objetivo:** Predecir la **Provincia** bas√°ndose en las m√©tricas de COVID-19.
    
    **Gaussian Naive Bayes** es un clasificador probabil√≠stico que asume que las caracter√≠sticas 
    siguen una distribuci√≥n normal (gaussiana). Es r√°pido y efectivo para clasificaci√≥n multiclase.
    """)
    
    # Par√°metros
    col1, col2 = st.columns(2)
    with col1:
        test_size = st.slider("Tama√±o del conjunto de prueba (%)", 10, 40, 20) / 100
    with col2:
        random_state = st.number_input("Semilla aleatoria", value=42, min_value=0)
    
    # Normalizaci√≥n opcional
    use_scaling = st.checkbox("Normalizar datos (StandardScaler)", value=True, 
                             help="Recomendado cuando las variables tienen diferentes escalas")
    
    if st.button("üöÄ Entrenar Modelo Gaussian Naive Bayes", type="primary"):
        # Preparar datos
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        # Normalizar si es necesario
        if use_scaling:
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            st.session_state['scaler'] = scaler
        else:
            X_train_scaled = X_train
            X_test_scaled = X_test
            st.session_state['scaler'] = None
        
        # Entrenar modelo
        model_supervised = GaussianNB()
        model_supervised.fit(X_train_scaled, y_train)
        
        # Predicciones
        y_pred = model_supervised.predict(X_test_scaled)
        
        # Calcular m√©tricas
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Guardar en session_state
        st.session_state['model_supervised'] = model_supervised
        st.session_state['X_train'] = X_train_scaled
        st.session_state['X_test'] = X_test_scaled
        st.session_state['y_test'] = y_test
        st.session_state['y_pred'] = y_pred
        st.session_state['metrics_supervised'] = {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1)
        }
        
        # Mostrar m√©tricas
        st.success("‚úÖ Modelo entrenado exitosamente!")
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("üéØ Accuracy", f"{accuracy:.4f}")
        col2.metric("üéØ Precision", f"{precision:.4f}")
        col3.metric("üéØ Recall", f"{recall:.4f}")
        col4.metric("üéØ F1-Score", f"{f1:.4f}")
        
        # Matriz de confusi√≥n
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_test, y_pred)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=province_names, yticklabels=province_names, ax=ax)
        ax.set_ylabel('Provincia Real')
        ax.set_xlabel('Provincia Predicha')
        ax.set_title('Matriz de Confusi√≥n - Predicci√≥n de Provincias')
        plt.tight_layout()
        st.pyplot(fig)
        
        # Reporte de clasificaci√≥n
        from sklearn.metrics import classification_report
        st.subheader("üìä Reporte Detallado por Provincia")
        report = classification_report(y_test, y_pred, target_names=province_names, output_dict=True)
        report_df = pd.DataFrame(report).transpose()
        st.dataframe(report_df.round(4))
    
    # Secci√≥n de predicci√≥n interactiva
    st.subheader("üîÆ Predicci√≥n Interactiva")
    
    if 'model_supervised' in st.session_state:
        st.markdown("**Ingresa los datos de COVID-19 para predecir la provincia:**")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            new_cases = st.number_input("Casos Nuevos", min_value=0, value=100, step=10)
            recoveries = st.number_input("Recuperados", min_value=0, value=50, step=10)
        
        with col2:
            deaths = st.number_input("Fallecidos", min_value=0, value=5, step=1)
            vaccinations = st.number_input("Vacunaciones", min_value=0, value=1000, step=100)
        
        with col3:
            hospitalized = st.number_input("Hospitalizados", min_value=0, value=20, step=5)
            tests_conducted = st.number_input("Tests Realizados", min_value=0, value=500, step=50)
        
        input_data = np.array([[new_cases, recoveries, deaths, vaccinations, hospitalized, tests_conducted]])
        
        # Normalizar si es necesario
        if st.session_state.get('scaler') is not None:
            input_data_scaled = st.session_state['scaler'].transform(input_data)
        else:
            input_data_scaled = input_data
        
        prediction = st.session_state['model_supervised'].predict(input_data_scaled)[0]
        prediction_proba = st.session_state['model_supervised'].predict_proba(input_data_scaled)[0]
        
        st.session_state['current_prediction'] = {
            'input': {
                'New_Cases': int(new_cases),
                'Recoveries': int(recoveries),
                'Deaths': int(deaths),
                'Vaccinations': int(vaccinations),
                'Hospitalized': int(hospitalized),
                'Tests_Conducted': int(tests_conducted)
            },
            'output_class': int(prediction),
            'output_label': province_names[prediction]
        }
        
        st.success(f"üó∫Ô∏è **Predicci√≥n: {province_names[prediction]}**")
        
        # Mostrar probabilidades
        st.subheader("Probabilidades por Provincia")
        prob_df = pd.DataFrame({
            'Provincia': province_names,
            'Probabilidad': prediction_proba
        }).sort_values('Probabilidad', ascending=False)
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.barh(prob_df['Provincia'], prob_df['Probabilidad'], color='#45B7D1')
        ax.set_xlabel('Probabilidad')
        ax.set_title('Probabilidad de Pertenencia a cada Provincia')
        plt.tight_layout()
        st.pyplot(fig)
        
        st.dataframe(prob_df.style.background_gradient(subset=['Probabilidad'], cmap='Blues'))
    else:
        st.info("üëÜ Entrena el modelo primero para usar esta funci√≥n")

# =============================================
# MODO 3: MODELO NO SUPERVISADO (K-MEANS)
# =============================================
elif mode == "üîç Modelo No Supervisado (K-Means)":
    st.header("üîç Modelo No Supervisado: K-Means Clustering")
    
    st.markdown("""
    **Objetivo:** Agrupar registros similares de COVID-19 en clusters sin usar las etiquetas de provincia.
    
    **K-Means** identifica patrones naturales en los datos agrupando registros con caracter√≠sticas similares.
    """)
    
    # Par√°metros
    col1, col2, col3 = st.columns(3)
    with col1:
        n_clusters = st.slider("N√∫mero de Clusters (K)", 2, 10, len(province_names))
    with col2:
        max_iter = st.number_input("M√°ximo de iteraciones", value=300, min_value=100)
    with col3:
        random_state = st.number_input("Semilla aleatoria", value=42, min_value=0, key='kmeans_seed')
    
    # Normalizaci√≥n (recomendada para K-Means)
    use_scaling = st.checkbox("Normalizar datos", value=True, 
                             help="Muy recomendado para K-Means debido a diferentes escalas")
    
    if st.button("üöÄ Entrenar Modelo K-Means", type="primary"):
        # Normalizar datos si es necesario
        if use_scaling:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            st.session_state['scaler_kmeans'] = scaler
        else:
            X_scaled = X
            st.session_state['scaler_kmeans'] = None
        
        # Entrenar modelo
        model_unsupervised = KMeans(n_clusters=n_clusters, max_iter=max_iter, 
                                   random_state=random_state, n_init=10)
        cluster_labels = model_unsupervised.fit_predict(X_scaled)
        
        # Calcular m√©tricas
        silhouette = silhouette_score(X_scaled, cluster_labels)
        davies_bouldin = davies_bouldin_score(X_scaled, cluster_labels)
        
        # Guardar en session_state
        st.session_state['model_unsupervised'] = model_unsupervised
        st.session_state['cluster_labels'] = cluster_labels
        st.session_state['X_scaled'] = X_scaled
        st.session_state['metrics_unsupervised'] = {
            'silhouette_score': float(silhouette),
            'davies_bouldin': float(davies_bouldin),
            'n_clusters': int(n_clusters)
        }
        
        # Mostrar m√©tricas
        st.success("‚úÖ Modelo K-Means entrenado exitosamente!")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("üìä Silhouette Score", f"{silhouette:.4f}", 
                   help="Rango: -1 a 1. Valores cercanos a 1 indican clusters bien definidos")
        col2.metric("üìä Davies-Bouldin Index", f"{davies_bouldin:.4f}",
                   help="Valores m√°s bajos indican mejor separaci√≥n de clusters")
        col3.metric("üéØ Clusters", n_clusters)
        
        # Interpretaci√≥n autom√°tica
        if silhouette > 0.5:
            st.success("‚úÖ Excelente separaci√≥n de clusters")
        elif silhouette > 0.3:
            st.info("‚ÑπÔ∏è Separaci√≥n de clusters aceptable")
        else:
            st.warning("‚ö†Ô∏è Los clusters tienen baja separaci√≥n")
        
        # Visualizaci√≥n de clusters
        st.subheader("üìä Visualizaci√≥n de Clusters")
        
        # An√°lisis de componentes principales para visualizaci√≥n 2D
        from sklearn.decomposition import PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # Gr√°fico 1: Clusters encontrados
        scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, 
                                  cmap='viridis', s=50, alpha=0.6, edgecolors='black', linewidths=0.5)
        axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} varianza)')
        axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} varianza)')
        axes[0].set_title('Clusters Encontrados por K-Means (PCA)')
        axes[0].grid(True, alpha=0.3)
        plt.colorbar(scatter1, ax=axes[0], label='Cluster')
        
        # Gr√°fico 2: Provincias reales (para comparaci√≥n)
        scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, 
                                  cmap='Set1', s=50, alpha=0.6, edgecolors='black', linewidths=0.5)
        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} varianza)')
        axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} varianza)')
        axes[1].set_title('Provincias Reales (Referencia)')
        axes[1].grid(True, alpha=0.3)
        plt.colorbar(scatter2, ax=axes[1], label='Provincia')
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # Distribuci√≥n de clusters
        st.subheader("üìä Distribuci√≥n de Registros por Cluster")
        
        col1, col2 = st.columns(2)
        
        with col1:
            cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
            fig, ax = plt.subplots(figsize=(8, 5))
            cluster_counts.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')
            ax.set_xlabel('Cluster')
            ax.set_ylabel('N√∫mero de Registros')
            ax.set_title('Tama√±o de cada Cluster')
            plt.tight_layout()
            st.pyplot(fig)
        
        with col2:
            # Tabla de distribuci√≥n
            cluster_df = pd.DataFrame({
                'Cluster': range(n_clusters),
                'Cantidad': [np.sum(cluster_labels == i) for i in range(n_clusters)],
                'Porcentaje': [f"{100*np.sum(cluster_labels == i)/len(cluster_labels):.2f}%" 
                             for i in range(n_clusters)]
            })
            st.dataframe(cluster_df, hide_index=True, use_container_width=True)
        
        # An√°lisis de caracter√≠sticas por cluster
        st.subheader("üìà Caracter√≠sticas Promedio por Cluster")
        
        df_with_clusters = df_processed.copy()
        df_with_clusters['Cluster'] = cluster_labels
        
        cluster_means = df_with_clusters.groupby('Cluster')[feature_names].mean()
        
        st.dataframe(cluster_means.round(2))
        
        # Heatmap de caracter√≠sticas por cluster
        fig, ax = plt.subplots(figsize=(12, 8))
        sns.heatmap(cluster_means.T, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax)
        ax.set_xlabel('Cluster')
        ax.set_ylabel('Caracter√≠sticas')
        ax.set_title('Perfil de cada Cluster (valores promedio)')
        plt.tight_layout()
        st.pyplot(fig)

# =============================================
# MODO 4: ZONA DE EXPORTACI√ìN
# =============================================
elif mode == "üíæ Zona de Exportaci√≥n":
    st.header("üíæ Zona de Exportaci√≥n (Dev Tools)")
    
    st.markdown("""
    Esta secci√≥n permite exportar los modelos entrenados y sus resultados en formatos
    que pueden ser consumidos por aplicaciones frontend (React) o reutilizados en Python.
    """)
    
    # Exportar Modelo Supervisado
    st.subheader("üì§ Exportar Modelo Supervisado (Gaussian Naive Bayes)")
    
    if 'model_supervised' in st.session_state:
        # Crear JSON
        json_supervised = {
            "model_type": "Supervised",
            "model_name": "Gaussian Naive Bayes",
            "algorithm": "GaussianNB",
            "dataset": "Pakistan COVID-19 Dataset",
            "features": feature_names,
            "target_classes": province_names.tolist(),
            "metrics": st.session_state['metrics_supervised'],
            "training_info": {
                "test_size": "20%",
                "normalization": "StandardScaler" if st.session_state.get('scaler') is not None else "None"
            }
        }
        
        if 'current_prediction' in st.session_state:
            json_supervised['sample_prediction'] = st.session_state['current_prediction']
        
        # Bot√≥n de descarga JSON
        json_str = json.dumps(json_supervised, indent=2)
        st.download_button(
            label="üì• Descargar JSON (Supervisado)",
            data=json_str,
            file_name="gaussian_nb_covid_results.json",
            mime="application/json"
        )
        
        with st.expander("üëÅÔ∏è Ver JSON"):
            st.code(json_str, language='json')
        
        # Bot√≥n de descarga PKL
        pkl_supervised = pickle.dumps(st.session_state['model_supervised'])
        st.download_button(
            label="üì• Descargar Modelo .pkl (Supervisado)",
            data=pkl_supervised,
            file_name="gaussian_nb_covid_model.pkl",
            mime="application/octet-stream"
        )
        
        # Exportar scaler si existe
        if st.session_state.get('scaler') is not None:
            pkl_scaler = pickle.dumps(st.session_state['scaler'])
            st.download_button(
                label="üì• Descargar Scaler .pkl",
                data=pkl_scaler,
                file_name="scaler.pkl",
                mime="application/octet-stream"
            )
        
        st.success("‚úÖ Modelo supervisado listo para exportar")
    else:
        st.warning("‚ö†Ô∏è Entrena el modelo supervisado primero en la secci√≥n correspondiente")
    
    st.divider()
    
    # Exportar Modelo No Supervisado
    st.subheader("üì§ Exportar Modelo No Supervisado (K-Means)")
    
    if 'model_unsupervised' in st.session_state:
        # Crear JSON
        json_unsupervised = {
            "model_type": "Unsupervised",
            "algorithm": "K-Means",
            "dataset": "Pakistan COVID-19 Dataset",
            "features": feature_names,
            "parameters": {
                "n_clusters": st.session_state['metrics_unsupervised']['n_clusters'],
                "max_iter": 300,
                "n_init": 10
            },
            "metrics": {
                "silhouette_score": st.session_state['metrics_unsupervised']['silhouette_score'],
                "davies_bouldin": st.session_state['metrics_unsupervised']['davies_bouldin']
            },
            "cluster_labels": st.session_state['cluster_labels'].tolist(),
            "cluster_distribution": {
                f"Cluster_{i}": int(np.sum(st.session_state['cluster_labels'] == i)) 
                for i in range(st.session_state['metrics_unsupervised']['n_clusters'])
            }
        }
        
        # Bot√≥n de descarga JSON
        json_str = json.dumps(json_unsupervised, indent=2)
        st.download_button(
            label="üì• Descargar JSON (No Supervisado)",
            data=json_str,
            file_name="kmeans_covid_results.json",
            mime="application/json"
        )
        
        with st.expander("üëÅÔ∏è Ver JSON"):
            st.code(json_str, language='json')
        
        # Bot√≥n de descarga PKL
        pkl_unsupervised = pickle.dumps(st.session_state['model_unsupervised'])
        st.download_button(
            label="üì• Descargar Modelo .pkl (No Supervisado)",
            data=pkl_unsupervised,
            file_name="kmeans_covid_model.pkl",
            mime="application/octet-stream"
        )
        
        st.success("‚úÖ Modelo no supervisado listo para exportar")
    else:
        st.warning("‚ö†Ô∏è Entrena el modelo no supervisado primero en la secci√≥n correspondiente")
    
    st.divider()
    
    # Instrucciones de uso
    st.subheader("üìñ Instrucciones de Uso")
    
    st.markdown("""
    ### C√≥mo usar los archivos exportados:
    
    **Archivos JSON:**
    - Pueden ser consumidos directamente por aplicaciones React/JavaScript
    - Contienen todas las m√©tricas y resultados del modelo
    - Formato legible y f√°cil de parsear
    
    **Archivos .pkl (Pickle):**
    - Contienen el modelo entrenado completo
    - Pueden ser cargados en Python para hacer predicciones
    
    ```python
    # Ejemplo de carga del modelo en Python
    import pickle
    
    # Cargar modelo
    with open('gaussian_nb_covid_model.pkl', 'rb') as f:
        model = pickle.load(f)
    
    # Hacer predicci√≥n
    nueva_prediccion = model.predict([[100, 50, 5, 1000, 20, 500]])
    ```
    
    **Integraci√≥n con React:**
    ```javascript
    // Leer el JSON en React
    fetch('gaussian_nb_covid_results.json')
        .then(response => response.json())
        .then(data => {
            console.log('Accuracy:', data.metrics.accuracy);
            console.log('Provincias:', data.target_classes);
        });
    ```
    """)

# Footer
st.sidebar.markdown("---")
st.sidebar.info(f"""
**Proyecto de Machine Learning**  
Dataset: COVID-19 Pakistan  
Modelos: Gaussian NB + K-Means  

**Caracter√≠sticas:**
- {len(df)} registros
- {len(province_names)} provincias
- {len(feature_names)} variables

Streamlit App v2.0
""")